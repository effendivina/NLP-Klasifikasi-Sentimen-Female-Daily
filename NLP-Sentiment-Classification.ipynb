{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNGSI PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFile(file_name):\n",
    "    with open(file_name, encoding=\"utf-8\") as csvfile: \n",
    "        next(csvfile)\n",
    "        rawArticles = csv.reader(csvfile, delimiter=',') \n",
    "        words = [] #array perkata akan menyimpan semua kalimat menjadi perkata\n",
    "        sentences = [] #array perkalimat menyimpan semua kalimat menjadi perkalimat\n",
    "        sentiment = []\n",
    "        for row in rawArticles:\n",
    "            sentences=row[3].lower()\n",
    "            words.append(sentences.split())\n",
    "            sentiment.append(row[5].lower())\n",
    "    return words, sentiment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStopWordsList(stopwordsfile):\n",
    "    stopwords=[]\n",
    "    file_stopwords = open(stopwordsfile,'r')\n",
    "    row = file_stopwords.readline()\n",
    "    while row:\n",
    "        word = row.strip()\n",
    "        stopwords.append(word)\n",
    "        row = file_stopwords.readline()\n",
    "    file_stopwords.close()\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmojiHandling(review):\n",
    "    emoji = []\n",
    "    for word in review:\n",
    "        #Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "        word = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))','POS',word)\n",
    "        \n",
    "        #Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "        word = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)','POS',word)\n",
    "\n",
    "        # Sad -- :-(, : (, :(, ):, )-:\n",
    "        word = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' NEG ', word)\n",
    "\n",
    "        # Cry -- :,(, :'(, :\"(, T_T\n",
    "        word = re.sub(r'(:,\\(|:\\'\\(|:\"\\(|T_T)', ' NEG ', word)\n",
    "\n",
    "        emoji.append(word)\n",
    "    return emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPunctHandling(review):\n",
    "    #menghilangkan tanda baca\n",
    "    preprocess_review = []\n",
    "    for word in review:\n",
    "        word = word.strip('\\'\"?!,.():;')\n",
    "\n",
    "        #mengkonversi huruf vocal lebih dari satu dan berurutan\n",
    "        word_character = re.compile(r\"(.)\\1+\", re.DOTALL)\n",
    "        word = word_character.sub(r\"\\1\\1\", word)\n",
    "\n",
    "        #menghilangkan tanda - & '\n",
    "        word = re.sub(r'(-|\\')','',word)\n",
    "\n",
    "        preprocess_review.append(word.lower())\n",
    "    return preprocess_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStemmingSentence(review):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureVector(review,stop_words_indo,stop_words_eng):\n",
    "    feature_vector = []\n",
    "    list_no = ['ga','engga','enggak','gak','nggak','ngga','tdk']\n",
    "    for word in review:\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", word) #menghilangkan karakter selain huruf didalam kata\n",
    "        if (word in stop_words_indo or val is None or word in stop_words_eng):\n",
    "            continue\n",
    "        else:\n",
    "            if word in list_no:\n",
    "                word = 'tidak'\n",
    "            feature_vector.append(word)\n",
    "    for_stemming = ' '.join(feature_vector)\n",
    "    return feature_vector, for_stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegativeHandling(review):\n",
    "    negative_review = []\n",
    "    for i in range(len(review)):\n",
    "        word = review[i]\n",
    "        if review[i-1] != 'tidak':\n",
    "            negative_review.append(word)\n",
    "        else:\n",
    "            word = 'tidak_'+word\n",
    "            negative_review.append(word)\n",
    "    return negative_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessReview(review):\n",
    "    return getPunctHandling(getEmojiHandling(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFreqDict(reviewHandled):\n",
    "    freqOfWord = {}\n",
    "    for sentence in reviewHandled:\n",
    "        for word in sentence:\n",
    "            if word in freqOfWord:\n",
    "                freqOfWord[word] += 1\n",
    "            else:\n",
    "                freqOfWord[word] = 1\n",
    "    file_key = open('keys.txt','w')\n",
    "    for key in freqOfWord.keys():\n",
    "        file_key.write(str(key))\n",
    "        file_key.write(\"\\n\")\n",
    "    file_key.close()\n",
    "    return freqOfWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureExtraction(review):\n",
    "    words = set(review)\n",
    "    features = {}\n",
    "    for word in feature_list.keys():\n",
    "        features['contains(%s)' % word] = (word in words) \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACA DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train, sentiments_train = openFile('50-Data-Train.csv')\n",
    "reviews_test, sentiments_test = openFile('5-Data-Test.csv')\n",
    "stop_words_indo = getStopWordsList('stopwordsindo.txt')\n",
    "stop_words_eng = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocess_reviews = []\n",
    "tokens = []\n",
    "reviews = []\n",
    "handled_reviews = []\n",
    "feature_list = []\n",
    "for review in corpus_train:\n",
    "    feature, review_for_stem = getFeatureVector(preprocessReview(review),stop_words_indo,stop_words_eng)\n",
    "    preprocess_reviews.append(getStemmingSentence(review_for_stem))\n",
    "\n",
    "for review in preprocess_reviews:\n",
    "    tokens.append(nltk.word_tokenize(review))\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    neg_handled_rev = getNegativeHandling(tokens[i])\n",
    "    handled_reviews.append(neg_handled_rev)\n",
    "    reviews.append((neg_handled_rev,sentiments_train[i]))\n",
    "\n",
    "feature_list = createFreqDict(handled_reviews)\n",
    "training_set = nltk.classify.util.apply_features(getFeatureExtraction,reviews)\n",
    "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDASI DATA TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "validation_test = []\n",
    "for review in reviews_test:\n",
    "    feature_classification, review_test_for_stem = getFeatureVector(review,stop_words_indo,stop_words_eng)\n",
    "    handled_reviews_test = getNegativeHandling(feature_classification)\n",
    "    classify_result = NBClassifier.classify(getFeatureExtraction(handled_reviews_test))\n",
    "    prediction.append((review,classify_result))\n",
    "    validation_test.append(classify_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['pembersih', 'yang', 'nagih', 'banget,', 'gak', 'ribet', 'pake', 'cleansing', 'milk', 'n', 'toner,', 'cetaphil', 'gentle', 'cleanser', 'ini', 'sengaja', 'aq', 'pake', 'buat', 'bersihin', 'muka', 'pake', 'kapas,', 'karena', 'kalo', 'pake', 'air', 'gak', 'ngluarin', 'busa', 'jd', 'berasa', 'ada', 'yang', 'kurang,', 'tp', 'aq', 'uda', 'repurchase', '3x', ',', 'suami', 'aq', 'juga', 'yang', 'awal', 'g', 'pernah', 'bersihin', 'muka,', 'sekarang', 'maunya', 'cuma', 'dibersihin', 'pake', 'cetaphil', 'gentle', 'cleanser', 'ini.'], 'positive')\n",
      "(['bener-bener', 'produk', 'facial', 'wash', 'paling', 'oke', 'kalo', 'menurut', 'saya.', 'sesuai', 'dengan', 'tagline', 'cetaphil', ':', 'every', 'age,', 'every', 'stage,', 'every', 'day,', 'produk', 'ini', 'emang', 'cocok', 'untuk', 'setiap', 'umur,', 'jenis', 'kulit', 'dan', 'perluu', 'digunakan', 'setiap', 'hari', 'oleh', 'setiap', 'orang.', 'tekstur', 'pelembabnya', 'berminyak,', 'awalnya', 'kaget', 'karena', 'pembersih', 'wajah', 'biasa', 'kan', 'biasanya', 'bentuknya', 'sabun.', 'pas', 'diapply', 'juga', 'engga', 'berbusa', 'tapi', 'muka', 'bersih', 'banget.', 'make', 'up', 'juga', 'hilang', 'walaupun', 'kalo', 'makeup', 'nya', 'berat', 'tetep', 'perlu', 'make', 'up', 'remover.', 'yang', 'jelas', 'cleanser', 'ini', 'sama', 'sekali', 'engga', 'bikin', 'muka', 'jadi', 'kering.', 'sangat', 'cocok', 'untuk', 'yang', 'kulitnya', 'kering', 'dan', 'sensitif.', 'untuk', 'kulit', 'berminyak', 'juga', 'cocok', 'karena', 'membuat', 'kulit', 'jadi', 'normal,', 'jerawat', 'juga', 'jarang', 'muncul..', 'the', 'best', 'deh.'], 'positive')\n",
      "(['dengan', 'kondisi', 'kulit', 'acne', 'prone', 'dan', 'moody', '(?)', 'dibutuhkan', 'much', 'effort', 'bagi', 'aku', 'untuk', 'caro', 'facial', 'wash.', 'susah', 'bgt', 'dpt', 'yg', 'cocok.', 'most', 'of', 'drugstore', 'products', 'bikin', 'break', 'out', 'bahkan', 'sampe', 'mengelupas.', 'bener-bener', 'harus', 'soapless,', 'and', 'mild', 'gitu.', 'setelah', 'sekian', 'tahun', 'bergantung', 'sama', 'sabun', 'dokter,', 'beraniin', 'diri', 'beli', 'ini', 'sabun.', 'and...ini', 'bener-bener', 'super', 'gentle', 'and', 'mild,', 'ga', 'berbusa,', 'berasa', 'bersih', 'dan', 'lembut', 'tiap', 'habis', 'cuci', 'muka,', 'dan', 'ga', 'bikin', 'khawatir', 'kl', 'cuci', 'muka', '3x', 'sehari', 'krn', 'super', 'lembut.', 'sayang,', 'packagingnya', 'bulky', 'abis', 'haha', 'but', 'it', 'doesnt', 'matter', 'sih.', 'definitely', 'would', 'repurchase,', 'walaupun', 'gatau', 'abisnya', 'kpn', 'krn', 'aku', 'punya', 'yg', 'setengah', 'liter', 'lol'], 'positive')\n",
      "(['pertama', 'pake', 'pas', 'jrwaran', 'parah..', 'liat', 'review', 'katanya', 'oke', 'buat', 'wjh', 'yg', 'lg', 'bermasalah..', 'tapi', 'nyatanya', 'ga', 'ada', 'efek', 'apa2', 'di', 'aku,', 'ngangkat', 'kotoran', 'jg', 'ngga', 'maksimal,', 'dan', 'ga', 'ngaruh', 'sm', 'sekali', 'dijerawat', 'malah', 'bikin', 'masalah', 'baru', 'kluar', 'bruntus..'], 'negative')\n",
      "(['kecewa', 'sama', 'produknya', ':((', 'pertama', 'kali', 'coba', 'yang', 'size', 'kecil', 'sampai', 'habis', 'terus', 'repurchase', 'yang', '500ml', 'karena', 'size', 'yang', 'kecil', 'engga', 'ngefek', 'apa2', 'ke', 'wajah', 'jadi', 'penasaran', 'beli', 'yang', '500ml.', 'dannnnn', 'sampe', 'habis', 'tetep', 'engga', 'efek', 'apa2.', 'padahal', 'semua', 'orang', 'recomment', 'cetaphil', 'karna', 'katanya', 'cocok', 'buat', 'acne', 'skin.', 'tapi', 'ternyata', 'tidaaaak', 'buatku.', 'kurang', 'suka', 'juga', 'sama', 'tekstur', 'dan', 'wanginya', 'sih', 'karena', 'setelah', 'dipake', 'kurang', 'greget', 'dimuka.', 'emang', 'katanya', 'bagus', 'sih', 'karena', 'engga', 'ada', 'aromanya.', 'mungkin', 'kita', 'belum', 'jodoh', 'cetaphil'], 'negative')\n"
     ]
    }
   ],
   "source": [
    "for sentiment in prediction:\n",
    "    print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HITUNG AKURASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_true = 0\n",
    "for k,val in enumerate(validation_test):\n",
    "    if val==sentiments_test[k]: \n",
    "        num_true+=1\n",
    "accuracy = (num_true/len(reviews_test))*100\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
